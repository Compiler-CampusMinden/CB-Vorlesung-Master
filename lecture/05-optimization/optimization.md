# Optimierung und Datenflussanalyse

> [!NOTE]
>
> <details>
>
> <summary><strong>üñá Weitere Unterlagen</strong></summary>
>
> - [Annotierte Folien: Optimierung und
>   Datenflussanalyse](https://github.com/Compiler-CampusMinden/AnnotatedSlides/blob/master/optimization.ann.ma.pdf)
>
> </details>

## Motivation

### Was geschieht hier?

    01  {
    02    var a;
    03    var b = 2;
    04    b = a;
    05  }

## Thema f√ºr heute: Optimierungen

### Was ist Optimierung in Compilern?

Ver√§ndern von Quellcode, Zwischencode oder Maschinencode eines Programms
mit dem Ziel,

- Laufzeit,
- Speicherplatz oder
- Energieverbrauch

zu verbessern.

### Was ist machbar?

Manche Optimierungen machen den Code nur in bestimmten F√§llen schneller,
kleiner oder stromsparender.

Den optimalen Code zu finden, ist oft NP-vollst√§ndig oder sogar
unentscheidbar.

- Heuristiken kommen zum Einsatz.

- Der Code wird verbessert, nicht in jedem Fall optimiert, manchmal auch
  verschlechtert.

- Der Einsatz eines Debuggers ist meist nicht mehr m√∂glich.

### Anforderungen an Optimierung

- sichere Transformationen durchf√ºhren

- m√∂glichst keine nachteiligen Effekte erzeugen

### Optimierung zur √úbersetzungszeit vs.¬†Optimierung zur Laufzeit

- Just-in-time-Compilierung (JIT), z. B. Java:

  Fast alle Optimierungsma√ünahmen finden in der virtuellen Maschine zur
  Laufzeit statt.

- Ahead-of-time-Compilierung (AOT), z. B. C:

  Der Compiler erzeugt Maschinencode, die Optimierung findet zur
  √úbersetzungszeit statt.

Beide haben ihre eigenen Optimierungsm√∂glichkeiten, es gibt aber auch
Methoden, die bei beiden einsetzbar sind.

### Welcher Code wird optimiert?

- Algebraische Optimierung: Transformationen des Quellcodes

- Maschinenunabh√§ngige Optimierung: Transformationen des Zwischencodes

- Maschinenabh√§ngige Optimierung: Transformationen des Assemblercodes
  oder Maschinencodes

Viele Transformationen sind auf mehr als einer Ebene m√∂glich. Wir wenden
hier die meisten auf den Zwischencode an.

### Welche Arten von Transformationen sind m√∂glich?

- Eliminierung unn√∂tiger Berechnungen

- Ersetzung von teuren Operationen durch kosteng√ºnstigere

### Basisbl√∂cke und Flussgraphen

**Def.:** Ein *Basisblock* ist eine Sequenz maximaler L√§nge von
Anweisungen, die immer hintereinander ausgef√ºhrt werden.

Ein Sprungbefehl kann nur der letzte Befehl eines Basisblocks sein.

**Def.:** Ein *(Kontroll)Flussgraph* $`G = (V, E)`$ ist ein Graph mit

$`V = \lbrace B_i \ \vert \ B_i \text{ ist ein Basisblock des zu compilierenden Programms} \rbrace`$,

$`E = \lbrace (B_i, B_j)\ \vert \text{ es gibt einen Programmlauf, in dem } B_j \text{ direkt hinter } B_i \text{ ausgef√ºhrt wird} \rbrace`$

### Beispiel

Hier entsteht ein Tafelbild.

### H√§ufig benutzte Strategie: Peephole-Optimierung

Ein Fenster mit wenigen Zeilen Inhalt gleitet √ºber den Quellcode,
Zwischencode oder den Maschinencode. Der jeweils sichtbare Code wird mit
Hilfe verschiedener Verfahren optimiert, wenn m√∂glich.

Peephole-Optimierung ist zun√§chst ein lokales Verfahren, kann aber auch
auf den gesamten Kontrollflussgraphen erweitert werden.

=\> Anwendung von Graphalgorithmen!

## Algebraische Optimierung

### Ersetzen von Teilb√§umen im AST durch andere B√§ume

        x = x*2         =>      x << 1

        x = x + 0       // k.w.
        x = x * 1       // k.w.

        x = x*0         =>      x = 0
        x = x*8         =>      x = x << 3

Sei $`s = 2^a + 2^b`$ die Summe zweier Zweierpotenzen:

        x = n*s         =>      (n << a) + (n << b)

Diese Umformungen k√∂nnen zus√§tzlich mittels Peephole-Optimierung in
sp√§teren Optimierungsphasen durchgef√ºhrt werden.

## Maschinenunabh√§ngige Optimierung

### Maschinenunabh√§ngige Optimierung

- lokal (= innerhalb eines Basisblocks), z. B. Peephole-Optimierung

  Einige Strategien sind auch global einsetzbar (ohne die sog.
  Datenflussanalyse s. u.)

- global, braucht nicht-lokale Informationen

  - meist unter Zuhilfenahme der Datenflussanalyse
  - Schleifenoptimierung

### Zwischencode (intermediate code); hier: Drei-Adress-Code

- registerbasiert
- Formen: `x = y op z, x = op z, x = y`
- tempor√§re Variablen f√ºr Zwischenergebnisse
- bedingte und unbedingte Spr√ºnge
- Pointerarithmetik f√ºr Indizierung

<!-- -->

    i = 0
    while(f[i] > 100)
        i = i + 1;

        i = 0
    L1: t1 = i * 8
        t2 = f + t1
        if t2 <= 100 goto L2
        t3 = i + 1
        i = t3
        goto L1
    L2: ...

## Lokale Optimierung

### Constant Folding und Common Subexpression elimination

- ‚Äú*Constant Folding*‚Äù: Auswerten von Konstanten zur Compile-Zeit

      x = 6 * 7         =>      x = 42
      if 2 > 0 jump L   =>      jump L

<!-- -->

- ‚Äú*Common Subexpression Elimination*‚Äù

      x = y + z
      ...
      a = y + z

  ersetze mit (falls in `...` keine weiteren Zuweisungen an `x`, `y`,
  `z` erfolgen)

      x = y + z
      ...
      a = x

### Elimination redundanter Berechnungen in einem Basisblock mitels DAGs

Hier werden sog. *DAG*s ben√∂tigt:

Ein DAG *directed acyclic graph* ist ein gerichteter, kreisfreier Graph.

DAGs werden f√ºr Berechnungen in Basisbl√∂cken generiert, um gemeinsame
Teilausdr√ºcke zu erkennen.

*Bsp.:* a = (b + c) \* (b + c) / 2

### Copy propagation

- ‚Äú*Copy Propagation*‚Äù

      x = y + z
      a = x
      b = 2*a

  ersetze mit

      x = y + z
      a = x
      b = 2*x

Wenn auf *a* vor seiner n√§chsten Zuweisung nicht mehr lesend zugegriffen
wird, kann *a* hier entfallen.

## Globale Optimierung

### Control Flow und Dead Code

- Kontrollfluss-Optimierungen

          if debug == 1 goto L1           if debug != 1 goto L2
          goto L2                         print debug info
      L1: print debug info            L2: ...
      L2: ...

<!-- -->

- Elimination of unreachable code

           goto L1                        L1: a = b+c
           ...
       L1: a = b+c

### Schleifenoptimierung

Loop unrolling:

            for i = 1 to 3                  print("1")
                print(i)                    print("2")
                                            print("3")

Code Hoisting:

- Invarianten vor die Schleife schieben

         x = 0                           x = 0
      L: a = n*7                         a = n*7
         x = x + a                    L: x = x + a
         if x<42 jump L                  if x<42 jump L

### Kombination zweier Verfahren

- Loop Unrolling (f√ºr eine Iteration), danach Common Subexpression
  Elimination

      while (cond) {                  if (cond) {
          body                            body
      }                                   while (cond) {
                                              body
                                          }
                                      }

### Datenflussanalyse

Die Datenflussanalyse (auf 3-Adress-Code) basiert auf dem Wissen der
Verf√ºgbarkeit von Variablen und Ausdr√ºcken am Anfang oder Ende von
Basisbl√∂cken, und zwar f√ºr alle m√∂glichen Programml√§ufe.

Man unterscheidet:

- Vorw√§rtsanalyse (in Richtung der Nachfolger eines Basisblocks)

- R√ºckw√§rtsanalyse (in Richtung der Vorg√§nger eines Basisblocks)

In beiden F√§llen gibt es zwei Varianten:

- any analysis: Es wird die Vereinigung von Informationen benachbarter
  Block ber√ºcksichtigt.

- all analysis: Es wird die Schnittmenge von Informationen benachbarter
  Block ber√ºcksichtigt.

### Forward-any-analysis

Diese Analyse wird zur Propagation von Konstanten und Variablen benutzt
und bildet sukzessive Mengen von Zeilen mit Variablendefinitionen.

``` math
out(B_i) = gen(B_i) \cup (in(B_i) - kill(B_i))
```

$`out(B_i)`$: alle Zeilennummern von Variablendefinitionen, die am Ende
von $`B_i`$ g√ºltig sind

$`in(B_i)`$: alle Zeilennummern von Variablendefinitionen, die am Ende
von Vorg√§ngerbl√∂cken von $`B_i`$ g√ºltig sind

$`gen(B_i)`$: alle Zeilennummern von letzten Variablendefinitionen in
$`B_i`$

$`kill(B_i)`$: alle Zeilennummern von Variablendefinitionen au√üerhalb
von $`B_i`$, die in $`B_i`$ √ºberschrieben werden

Zun√§chst ist $`in(B_1) = \emptyset`$, danach ist
$`in(B_i) = \bigcup  out(B_j)`$ mit $`B_j`$ ist Vorg√§nger von $`B_i`$.

### Forward-all-analysis

Diese Analyse wird zur Berechnung verf√ºgbarer Ausdr√ºcke der Form
$`x = y\ op\ z`$ f√ºr die Eliminierung redundanter Berechnungen benutzt
und bildet sukzessive Mengen von Ausdr√ºcken.

``` math
out(B_i) = gen(B_i) \cup (in(B_i) - kill(B_i))
```

$`out(B_i)`$: alle am Ende von $`B_i`$ verf√ºgbaren Ausdr√ºcke

$`in(B_i)`$: alle Ausdr√ºcke, die am Anfang von $`B_i`$ verf√ºgbar sind

$`gen(B_i)`$: alle in $`B_i`$ berechneten Ausdr√ºcke

$`kill(B_i)`$: alle Ausdr√ºcke $`x\ op\ y`$ mit einer Definition von
$`x`$ oder $`y`$ in $`B_i`$ und $`x\ op\ y`$ ist nicht in $`B_i`$

Zun√§chst ist $`gen(B_1) = \emptyset`$, danach ist
$`in(B_i) = \bigcap  out(B_j)`$ mit $`B_j`$ ist Vorg√§nger von $`B_i`$.

### Backward-any-analysis

Diese Analyse dient der Ermittlung von lebenden und toten Variablen (f√ºr
die Registerzuweisung) und bildet sukzessive Mengen von Variablen.

``` math
in(B_i) = gen(B_i) \cup (out(B_i) - kill(B_i))
```

$`out(B_i)`$: alle Variablen, die am Ende von $`B_i`$ lebendig sind

$`in(B_i)`$: alle Variablen, die am Ende von Vorg√§ngerbl√∂cken von
$`B_i`$ lebendig sind

$`gen(B_i)`$: alle Variablen, deren erstes Vorkommen auf der echten
Seite einer Zuweisung steht

$`kill(B_i)`$: alle Variablen, denen in $`B_i`$ Werte zugewiesen werden.

Zun√§chst ist $`out(B_n) = \emptyset`$, danach ist
$`out(B_i) = \bigcup in(B_j)`$ mit $`B_j`$ ist Nachfolger von $`B_i`$.

### Backward-all-analysis

Diese Analyse wird zur Berechnung von ‚Äúvery busy‚Äù Ausdr√ºcken der Form
$`x = y\ op\ z`$, die auf allen m√∂glichen Wegen im Flussgraphen vom
aktuellen Basisblock aus mindestens einmal benutzt werden. Ausdr√ºcke
sollten dort berechnet werden, wo sie very busy sind, um den Code k√ºrzer
zu machen.

``` math
in(B_i) = gen(B_i) \cup (out(B_i) - kill(B_i))
```

$`out(B_i)`$: alle Ausdr√ºcke $`x\ op\ y`$, die am Ende von $`B_i`$ very
busy sind

$`in(B_i)`$: alle Ausdr√ºcke, die am Anfang von $`B_i`$ very busy sind

$`gen(B_i)`$: alle in $`B_i`$ benutzen Ausdr√ºcke

$`kill(B_i)`$: alle Ausdr√ºcke $`x\ op\ y`$, deren Operanden in $`B_i`$
nicht redefiniert werden.

Zun√§chst ist $`out(B_n) = \emptyset`$, danach ist
$`out(B_i) = \bigcap in(B_j)`$ mit $`B_j`$ ist Nachfolger von $`B_i`$.

## Maschinenabh√§ngige Optimierung

### Elimination redundanter Lade-, Speicher- und Sprungoperationen

        LD a, R0
        ST R0, a        // k.w.

            goto L1         goto L2
            ...             ...
        L1: goto L2     L1: goto L2

### Register Allocation: Liveness Analysis

    a = b + c
    d = a + b
    e = d - 1

`a`, `d`, `e` k√∂nnen auf **ein** Register abgebildet werden!

    r1 = r2 + r3
    r1 = r1 + r2
    r1 = r1 - 1

=\> `a` und `d` sind nach Gebrauch ‚Äú*tot*‚Äù

### Berechnung der minimal ben√∂tigten Anzahl von Registern

=\> Liveness-Graph, F√§rbungsproblem f√ºr Graphen!

Es wird ein Graph $`G = (V, E)`$ erzeugt mit

$`V = \lbrace v \ \vert \ v \text{ ist eine ben√∂tigte Variable} \rbrace`$
und
$`E = \lbrace (v_1, v_2)\ \vert \ v_1  \text{ und } v_2 \text{ sind zur selben Zeit "lebendig"} \rbrace`$

Heuristisch wird jetzt die minimale Anzahl von Farben f√ºr Knoten
bestimmt, bei der benachbarte Knoten nicht dieselbe Farbe bekommen.

=\> Das Ergebnis ist die Zahl der ben√∂tigten Register.

### Und wenn man nicht so viele Register zur Verf√ºgung hat?

Registerinhalte tempor√§r in den Speicher auslagern (‚Äú*Spilling*‚Äù).

Kandidaten daf√ºr werden mit Heuristiken gefunden, z. B. Register mit
vielen Konflikten (= Kanten) oder Register mit selten genutzten
Variablen.

In Schleifen genutzte Variablen werden eher nicht ausgelagert.

## Optimierung zur Reduzierung des Energieverbrauchs

### Energieverbrauch verschiedener Maschinenbefehle

Maschinenoperationen, die nur auf Registern arbeiten, verbrauchen die
wenigste Energie.

Operationen, die nur lesend auf Speicherzellen zugreifen, verbrauchen
ca. ein Drittel mehr Energie.

Operationen, die Speicherzellen beschreiben, ben√∂tigen zwei Drittel mehr
Energie als die Operationen ausschlie√ülich auf Register.

### Energieeinsparung durch laufzeitbezogene Optimierung

K√ºrzere Programmlaufzeiten f√ºhren in der Regel auch zu
Energieeinsparungen.

**gcc -O1** spart 2% bis 70% (durchschnittlich 20%) Energie

Umgekehrt: Energiebezogene Optimierung f√ºhrt in der Regel zu k√ºrzeren
Laufzeiten.

### Prozessorspannung variieren

Viele Prozessoren erm√∂glichen es, die Betriebsspannung per
Maschinenbefehl zu ver√§ndern.

Eine h√∂here Spannung bewirkt eine proportionale Steigerung der
Prozessorgeschwindigkeit und des flie√üenden Stroms, aber einen
quadratischen Anstieg des Energieverbrauchs.
$`(P = U \times I, U = R \times I)`$

Folgendes kann man ausnutzen:

Die Verringerung der Spannung um 20% f√ºhrt zu einer um 20% geringeren
Prozessorgeschwindigkeit, d.¬†h. das Programm braucht 25% mehr Zeit,
verbraucht aber 36% $`(1-(1-0,2)^2)`$ weniger Energie.

=\> Wenn das Programm durch Optimierung um 25% schneller wird und die
Prozessorspannung um 20% verringert wird, ver√§ndert sich die Laufzeit
des Programms nicht, man spart aber 36% Energie.

## Wrap-Up

### Wrap-Up

- Verschiedene Optimierungsverfahren auf verschiedenen Ebenen, Peephole
- Datenflussanalyse
- Senkung des Energieverbrauchs durch Optimierung

## üìñ Zum Nachlesen

- G√ºting ([1999](#ref-G√ºting1999)): Kapitel 8
- Grune u.¬†a. ([2012](#ref-Grune2012)): Kapitel 9.3
- Aho u.¬†a. ([2023](#ref-Aho2023)): Kapitel 9
- Mogensen ([2017](#ref-Mogensen2017)): Kapitel 8, 10 und 11

> [!NOTE]
>
> <details>
>
> <summary><strong>‚úÖ Lernziele</strong></summary>
>
> - k1: Ich kenne verschiedene algebraische Optimierungen
> - k1: Ich kenne verschiedene maschinenunabh√§ngige Optimierungen
> - k1: Ich kenne verschiedene maschinenabh√§ngige Optimierungen
> - k1: Ich kenne Datenflussanalyse auf 3-Adress-Code
>
> </details>

------------------------------------------------------------------------

> [!NOTE]
>
> <details>
>
> <summary><strong>üëÄ Quellen</strong></summary>
>
> <div id="refs" class="references csl-bib-body hanging-indent"
> entry-spacing="0">
>
> <div id="ref-Aho2023" class="csl-entry">
>
> Aho, A. V., M. S. Lam, R. Sethi, J. D. Ullman, und S. Bansal. 2023.
> *Compilers: Principles, Techniques, and Tools, Updated 2nd Edition by
> Pearson*. Pearson India.
> <https://learning.oreilly.com/library/view/compilers-principles-techniques/9789357054881/>.
>
> </div>
>
> <div id="ref-Grune2012" class="csl-entry">
>
> Grune, D., K. van Reeuwijk, H. E. Bal, C. J. H. Jacobs, und K.
> Langendoen. 2012. *Modern Compiler Design*. Springer.
>
> </div>
>
> <div id="ref-G√ºting1999" class="csl-entry">
>
> G√ºting, R. H. 1999. *√úbersetzerbau: Techniken, Werkzeuge,
> Anwendungen*. Springer.
>
> </div>
>
> <div id="ref-Mogensen2017" class="csl-entry">
>
> Mogensen, T. 2017. *Introduction to Compiler Design*. Springer.
> <https://doi.org/10.1007/978-3-319-66966-3>.
>
> </div>
>
> </div>
>
> </details>

------------------------------------------------------------------------

<img src="https://licensebuttons.net/l/by-sa/4.0/88x31.png" width="10%">

Unless otherwise noted, this work is licensed under CC BY-SA 4.0.

<blockquote><p><sup><sub><strong>Last modified:</strong> 4353924 (lecture: rework outcomes (05/Optimierung), 2025-08-21)<br></sub></sup></p></blockquote>
